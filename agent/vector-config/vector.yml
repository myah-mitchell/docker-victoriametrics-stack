data_dir: "/var/lib/vector"

api:
  enabled: true
  address: 0.0.0.0:8686

sources:
  # -------------------------------------------------------------------------
  # Source: Docker Socket
  # -------------------------------------------------------------------------
  docker_socket:
    type: docker_logs
    # 'auto_partial_merge' is CRITICAL. Docker often splits long JSON lines 
    # (like Java stack traces) into 16KB chunks. This setting reassembles them 
    # before Vector tries to parse them, preventing "invalid JSON" errors.
    auto_partial_merge: true
  
  host_journald:
    type: "journald"
    journal_directory: "/var/log/journal"

  host_files:
    type: "file"
    include:
      - "/var/log/**/*.log"
    exclude:
      - "/var/log/journal/**/*" # Exclude journald binary files
      - "/var/log/traefik/**/*" # Exclude if handled separately or via Docker
      - "/var/log/auth.log"     # Redundant: Covered by host_journald
      - "/var/log/kern.log"     # Redundant: Covered by host_journald
  
  # Internal self-monitoring metrics for Vector
  vector_metrics:
    type: internal_metrics

transforms:
  # -------------------------------------------------------------------------
  # 1. Routing: Multiline Handling
  # -------------------------------------------------------------------------
  # We separate 'logrotate' logs because they contain indented config blocks
  # that need to be merged into a single log event. All other logs go to 'standard'.
  route_multiline:
    type: route
    inputs: ["docker_socket"]
    route:
      # Use 'contains' (string check) instead of 'match' (regex) to avoid VRL syntax errors in one-liners
      logrotate: 'contains(string(.container_name) ?? "", "logrotate")'
      standard: '!contains(string(.container_name) ?? "", "logrotate")'

  # -------------------------------------------------------------------------
  # 2. Reduction: Multiline Bundling
  # -------------------------------------------------------------------------
  # This merges the indented lines of output into the preceding header line.
  reduce_multiline:
    type: reduce
    inputs: ["route_multiline.logrotate"]
    # Group by container only (merging stdout/stderr together so banners and details stay coherent)
    group_by: ["container_name"]
    merge_strategies:
      message: concat_newline
    # Logic: Start a new log event ONLY if the line starts with a Number (The Timestamp).
    # 1. Slice the first character (index 0 to 1).
    # 2. Add '?? "x"' fallback because slice() crashes on empty strings.
    # 3. Convert to int. If it's a digit (0-9), it returns a number (truthy for start).
    # 4. If it's a space, letter, or symbol, it returns null, meaning "merge this line".
    starts_when: 'is_integer(to_int(slice(string(.message) ?? "", 0, 1) ?? "x") ?? null)'
    expire_after_ms: 2000 # Flush the buffer if no new lines arrive for 2 seconds

  # -------------------------------------------------------------------------
  # 3. Processing: Parsing & Normalization (The Main Logic)
  # -------------------------------------------------------------------------
  process_docker:
    type: "remap"
    # Combine the standard stream and the now-reduced logrotate stream
    inputs: ["route_multiline.standard", "reduce_multiline"]
    source: |
      # --- 1. Metadata Cleanup ---
      # Docker container names often come with a leading slash (e.g., "/traefik").
      # We remove it for cleaner querying in VictoriaLogs.
      .container_name = replace(string(.container_name) ?? "", "/", "")

      # --- 2. Structured Log Parsing ---
      # First, try to parse the message as JSON.
      .message = strip_whitespace(string(.message) ?? "")
      structured, err = parse_json(.message)

      # If JSON fails, try Key-Value (LogFmt). Grafana and many Go apps use this.
      # CRITICAL: We MUST skip this for containers known to use specific text formats (handled by Regex below).
      # Otherwise, parse_key_value() aggressively parses text logs (like HAProxy) as boolean keys (e.g. "200": true),
      # corrupting the log and preventing the Regex fallback from running.
      if err != null {
        is_known_text_container = match(.container_name, r'socket-proxy|traefik-kop|cadvisor|logrotate|postgres|ferretdb')
        if !is_known_text_container {
           structured, err = parse_key_value(.message)
        }
      }

      if err == null {
        # === HAPPY PATH: STRUCTURED LOGS (JSON OR Key-Value) ===
        # Merge the parsed fields to the root of the event.
        merged, merge_err = merge(., structured)
        
        if merge_err == null {
          . = merged
          
          # A. Message Normalization
          # Move 'msg', 'log', or 'message' fields to the standard 'message' field
          if exists(.msg) {
            .message = .msg
            del(.msg)
          } else {
            if exists(.log) {
              .message = .log
              del(.log)
            }
          }

          # B. Level Normalization
          # Scan for common level fields (severity, lvl, log_level).
          found_levels = compact([.level, .severity, .lvl, .log_level])
          if length(found_levels) > 0 {
             raw_level = string(found_levels[0]) ?? "UNKNOWN"
             # Clean up levels like "\"info\"" (remove quotes) and uppercase them.
             clean_level = replace(raw_level, "\"", "")
             .level = upcase(clean_level)
          }

          # C. Timestamp Normalization
          # Handle 'ts' (VM), 't' (Grafana), or fix 'timestamp' (Vector).
          ts_val = null
          ts_field = null

          if exists(.ts) { 
            ts_val = .ts
            ts_field = "ts" 
          } else {
            if exists(.t) { 
              ts_val = .t
              ts_field = "t" 
            } else {
              if is_string(.timestamp) { 
                ts_val = .timestamp
                ts_field = "timestamp" 
              }
            }
          }

          if ts_val != null {
            # Parse ISO8601/RFC3339 format (e.g. 2026-01-25T...)
            parsed_ts, ts_err = parse_timestamp(ts_val, "%+") 
            if ts_err == null {
              .timestamp = parsed_ts
              # Remove original field to avoid duplication (unless it was .timestamp, which we overwrote)
              if ts_field != "timestamp" {
                 del(."{{ts_field}}")
              }
            }
          }

          # D. Safety Check
          # VictoriaLogs requires 'message' to be a string.
          if !is_string(.message) {
            .message = encode_json(.message)
          }
        } else {
          # Log internal error if JSON merge fails (e.g. type conflict)
          log("Structured Merge failed for " + .container_name + ": " + (string(merge_err) ?? "unknown error"), level: "error")
        }
      } else {
        # === FALLBACK PATH: TEXT LOGS (REGEX) ===
        # If structured parsing failed, we check specific container types and apply Regex.
        
        # --- Socket-Proxy (HAProxy Format) ---
        # Format: 10.0.1.2:33317 [06/Feb/2009:12:14:14.655] http-in static/srv1 10/0/30/69/109 200 2750 - - ---- 1/1/1/1/0 0/0 "GET /index.html HTTP/1.1"
        if match(.container_name, r'socket-proxy') {
          haproxy_parts, regex_err = parse_regex(.message, r'^(?P<client_ip>[a-fA-F0-9:.]+):(?P<client_port>\d+) \[(?P<proxy_ts>[^\]]+)\] (?P<frontend_name>\S+) (?P<backend_name>[^/]+)/(?P<server_name>\S+) (?P<time_request>\d+)/(?P<time_queue>\d+)/(?P<time_connecting>\d+)/(?P<time_response>\d+)/(?P<time_active>\d+) (?P<status_code>\d+) (?P<bytes_read>\d+) (?P<captured_request_cookie>\S+) (?P<captured_response_cookie>\S+) (?P<termination_state>\S+) (?P<actconn>\d+)/(?P<feconn>\d+)/(?P<beconn>\d+)/(?P<src_conn>\d+)/(?P<retries>\d+) (?P<src_queue>\d+)/(?P<backend_queue>\d+) (?P<captured_headers>[^"]*)"(?P<method>\w+) (?P<path>\S+) (?P<http_version>[^"]+)"[ ]{0,1}(?P<ssl_fields>.*$)')

          if regex_err != null {
            haproxy_parts, regex_err = parse_regex(.message, r'^(?P<client_ip>[a-fA-F0-9:.]+):(?P<client_port>\d+) \[(?P<proxy_ts>[^\]]+)\] (?P<frontend_name>[^/^]+)/(?P<bind_name>\S+) (?P<error>.*)$')
          }

          if regex_err == null {
            . = merge(., haproxy_parts)
          
            # Parse custom HAProxy timestamp (e.g. 24/Jan/2026:18:10:04.947)
            parsed_ts, ts_err = parse_timestamp(.proxy_ts, "%d/%b/%Y:%H:%M:%S.%f")
            if ts_err == null { .timestamp = parsed_ts }

            # Infer Level from HTTP Status Code
            sc = to_int(haproxy_parts.status_code) ?? 200
            .level = "INFO"
            if sc >= 400 { .level = "WARN" }
            if sc >= 500 { .level = "ERROR" }
          }
        }

        # --- Traefik-Kop (Pipe Delimited) ---
        # Format: Time | Status | Duration | IP | Method | Path | UA
        if match(.container_name, r'traefik-kop') {
           kop_parts, regex_err = parse_regex(.message, r'^\s*(?P<time>\S+)\s+\|\s+(?P<status>\d+)\s+\|\s+(?P<duration>[^|]+)\s+\|\s+(?P<client_ip>[^|]+)\s+\|\s+(?P<method>[^|]+)\s+\|\s+(?P<path>[^|]+)\s+\|\s+(?P<user_agent>.*)$')

           if regex_err == null {
             # Clean whitespace around extracted fields
             kop_parts.duration = strip_whitespace!(kop_parts.duration)
             kop_parts.client_ip = strip_whitespace!(kop_parts.client_ip)
             kop_parts.method = strip_whitespace!(kop_parts.method)
             kop_parts.path = strip_whitespace!(kop_parts.path)
             kop_parts.user_agent = strip_whitespace!(kop_parts.user_agent)

             . = merge(., kop_parts)

             # Infer Level from Status
             sc = to_int(kop_parts.status) ?? 200
             .level = "INFO"
             if sc >= 400 { .level = "WARN" }
             if sc >= 500 { .level = "ERROR" }
           }
        }

        # --- cAdvisor (Kubernetes/klog) ---
        # Format: I0125 01:22:10... 1 file.go:220] Message...
        if match(.container_name, r'cadvisor') {
          klog_parts, klog_err = parse_klog(.message)
          if klog_err == null {
            . = merge(., klog_parts)
          }
        }

        # --- Logrotate (Custom Format) ---
        # Format: 2026-01-24 20:02:32 INFO [script:24] Message...
        if match(.container_name, r'logrotate') {
           # Capture using (?s:.*) to allow matching newlines (from reduced multiline logs)
           lr_parts, regex_err = parse_regex(.message, r'^(?P<ts>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\s+(?P<lvl>\w+)\s+\[(?P<src>[^\]]+)\]\s+(?P<msg>(?s:.*))$')
           
           if regex_err == null {
             .level = upcase!(lr_parts.lvl)
             .message = lr_parts.msg
             .source_origin = lr_parts.src
             
             # Parse timestamp (e.g. 2026-01-24 20:02:32)
             parsed_ts, ts_err = parse_timestamp(lr_parts.ts, "%Y-%m-%d %H:%M:%S")
             if ts_err == null {
               .timestamp = parsed_ts
             }
           } else {
             # Mark banners/configs as INFO so they aren't labeled UNKNOWN/ERROR
             .level = "INFO"
           }
        }

        # --- Postgres (Default Text Format) ---
        # Format: 2026-01-25 18:42:02.917 CST [30] LOG:  message...
        if match(.container_name, r'postgres') {
           pg_parts, regex_err = parse_regex(.message, r'^(?P<ts>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d+)\s+(?P<tz>\w+)\s+\[(?P<pid>\d+)\]\s+(?P<lvl>\w+):\s+(?P<msg>.*)$')
           
           if regex_err == null {
             .message = pg_parts.msg
             .postgres_pid = pg_parts.pid
             .level = upcase!(pg_parts.lvl)
             
             # Map Postgres specific levels to standard ones if needed
             if .level == "LOG" { .level = "INFO" }
             
             # Parse timestamp (e.g. 2026-01-25 18:42:02.917)
             parsed_ts, ts_err = parse_timestamp(pg_parts.ts, "%Y-%m-%d %H:%M:%S.%f")
             if ts_err == null {
               .timestamp = parsed_ts
             }

             # Extract Checkpoint Metrics (buffers, timing, size)
             # Example msg: "checkpoint complete: wrote 25 buffers ... write=2.508 s ... distance=147 kB ..."
             # Fix E110 & E651: .message is guaranteed string (from pg_parts.msg), so we don't need '?? ""'
             msg_str = string(.message) ?? ""
             if starts_with(msg_str, "checkpoint complete:") {
                cp, cp_err = parse_regex(msg_str, r'wrote (?P<buffers>\d+) buffers.*write=(?P<write_s>[\d\.]+) s, sync=(?P<sync_s>[\d\.]+) s, total=(?P<total_s>[\d\.]+) s; sync files=(?P<files>\d+).*distance=(?P<dist_kb>\d+) kB, estimate=(?P<est_kb>\d+) kB')
                if cp_err == null {
                  .postgres_buffers_written = to_int(cp.buffers) ?? null
                  .postgres_write_s = to_float(cp.write_s) ?? null
                  .postgres_sync_s = to_float(cp.sync_s) ?? null
                  .postgres_total_s = to_float(cp.total_s) ?? null
                  .postgres_sync_files = to_int(cp.files) ?? null
                  .postgres_distance_kb = to_int(cp.dist_kb) ?? null
                  .postgres_estimate_kb = to_int(cp.est_kb) ?? null
                }
             }
           }
        }

        # --- FerretDB ---
        # Format: 2026-01-26T02:47:41.631Z	INFO	middleware/dispatcher.go:131	Command handled	{"command":"find",...}
        if match(.container_name, r'ferretdb') {
           ferret_parts, regex_err = parse_regex(.message, r'^(?P<ts>\S+)\s+(?P<lvl>\w+)\s+(?P<src>\S+)\s+(?P<msg>.*)\s+(?P<json_context>\{.*\})$')
           
           if regex_err == null {
             .timestamp = parse_timestamp(ferret_parts.ts, "%+") ?? .timestamp
             .level = ferret_parts.lvl
             .source_origin = ferret_parts.src
             .message = ferret_parts.msg
             
             # Extract structured context
             ctx, json_err = parse_json(ferret_parts.json_context)
             if json_err == null {
               merged_ferret, merge_err = merge(., ctx)
               if merge_err == null {
                 . = merged_ferret
               }
             }
           }
        }
      }

      # --- 4. Final Cleanup ---
      
      # Set Default Level: If no logic above found a level, mark as UNKNOWN.
      # This helps identify new containers that need parsing rules.
      if !exists(.level) {
        .level = "UNKNOWN"
      }

      # VictoriaLogs Timestamp:
      # Format the final valid .timestamp object into an RFC3339 string field '_time'.
      # We assume .timestamp is a Timestamp object, but if parsing failed earlier, it might still be a String.
      # We check the type to prevent "expected timestamp, got string" crashes.
      # Fix: Replaced 'else if' with nested 'else { if ... }' to avoid VRL syntax issues in this context.
      if is_timestamp(.timestamp) {
        ._time = format_timestamp!(.timestamp, format: "%+")
      } else {
        if is_string(.timestamp) {
          # If it is a string, pass it raw. VictoriaLogs handles many string formats automatically.
          ._time = .timestamp
        } else {
          # Fallback to capture time if no valid timestamp exists
          ._time = format_timestamp!(now(), format: "%+")
        }
      }

  # -------------------------------------------------------------------------
  # 5. Journald Normalization
  # -------------------------------------------------------------------------
  process_journald:
    type: "remap"
    inputs: ["host_journald"]
    source: |
      # Map Journald fields to match the Sink's expectations (container_name, _time)
      .container_name = ._SYSTEMD_UNIT || .SYSLOG_IDENTIFIER || "host-system"
      
      if is_timestamp(.timestamp) {
        ._time = format_timestamp!(.timestamp, format: "%+")
      }

      # Map Syslog Priority to Human Readable Level
      # 0=Emerg, 1=Alert, 2=Crit, 3=Err, 4=Warning, 5=Notice, 6=Info, 7=Debug
      p = to_int(.PRIORITY) ?? 6
      if p <= 3 {
        .level = "ERROR"
      } else if p == 4 {
        .level = "WARN"
      } else if p == 5 || p == 6 {
        .level = "INFO"
      } else {
        .level = "DEBUG"
      }

  # -------------------------------------------------------------------------
  # 6. Host File Normalization
  # -------------------------------------------------------------------------
  process_files:
    type: "remap"
    inputs: ["host_files"]
    source: |
      # Create a stream name from the filename (e.g., /var/log/syslog -> host-syslog)
      # We use the 'file' metadata key provided by the source
      path_parts = parse_regex(.file, r'^/var/log/(?P<name>.*)$') ?? {"name": "unknown"}
      .container_name = "host-" + replace(path_parts.name, "/", "-")
      
      ._time = format_timestamp!(now(), format: "%+")

sinks:
  # -------------------------------------------------------------------------
  # Sink: VictoriaLogs (via VLAgent)
  # -------------------------------------------------------------------------
  victorialogs:
    type: "elasticsearch"
    inputs: ["process_docker", "process_journald", "process_files"]
    endpoints: ["http://vlagent:9429/insert/elasticsearch/"]
    mode: "bulk" # Use Bulk API for performance (compresses many logs into one request)
    api_version: "v8"
    compression: "gzip"
    healthcheck:
      enabled: false

    # Batching: Send every 1MB or every 1 second, whichever comes first.
    batch:
      max_bytes: 1048576
      timeout_secs: 1

    request:
      headers:
        # VL Headers to control indexing behavior
        VL-Stream-Fields: "container_name"  # High cardinality field for stream definition
        VL-Time-Field: "_time"              # Use our computed timestamp string
        VL-Msg-Field: "message"             # The field containing the log text
        AccountID: "0"
        ProjectID: "0"

  # Sink: VictoriaMetrics (Internal Vector Health)
  victoriametrics:
    type: prometheus_remote_write
    endpoint: http://vmagent:8429/api/v1/write
    inputs: [vector_metrics]
    healthcheck:
      enabled: false